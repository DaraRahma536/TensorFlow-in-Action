{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMywAjj2evnO9we1jWjvdwJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaraRahma536/TensorFlow-in-Action/blob/main/Chapter_08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapter 8: Image Segmentation dengan DeepLab v3**\n",
        "Chapter ini membahas implementasi semantic image segmentation menggunakan model DeepLab v3 dengan dataset PASCAL VOC 2012. Fokus utama meliputi:\n",
        "* Persiapan data untuk tugas segmentasi\n",
        "* Implementasi pipeline tf.data yang efisien\n",
        "* Arsitektur DeepLab v3 dengan atrous convolution dan ASPP\n",
        "* Fungsi loss dan metrik kustom untuk segmentasi\n",
        "* Pelatihan dan evaluasi model"
      ],
      "metadata": {
        "id": "2inY1ugkUj7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Jenis Segmentasi**\n",
        "---\n",
        "### **A. Semantic Segmentation**\n",
        "* Setiap pixel diklasifikasikan ke kategori objek\n",
        "* Objek yang sama jenisnya mendapat label sama (misal: semua orang = satu class)\n",
        "\n",
        "### **B. Instance Segmentation**\n",
        "* Setiap objek individu dipisahkan meskipun jenisnya sama\n",
        "* Lebih sulit dari semantic segmentation\n",
        "* Dataset yang digunakan: PASCAL VOC 2012 (22 kelas termasuk background)"
      ],
      "metadata": {
        "id": "Oe1h1mEqU_qk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Persiapan Data Pipeline dengan ```tf.data```**\n",
        "---\n",
        "### **A. Memuat Data Segmentasi**"
      ],
      "metadata": {
        "id": "jN-vvyr5VcJV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj_55AArUiOl"
      },
      "outputs": [],
      "source": [
        "# Listing 8.1: Download dataset\n",
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "\n",
        "if not os.path.exists(os.path.join('data','VOCtrainval_11-May-2012.tar')):\n",
        "    url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\"\n",
        "    r = requests.get(url)\n",
        "\n",
        "    if not os.path.exists('data'):\n",
        "        os.mkdir('data')\n",
        "\n",
        "    with open(os.path.join('data','VOCtrainval_11-May-2012.tar'), 'wb') as f:\n",
        "        f.write(r.content)\n",
        "\n",
        "# Ekstrak jika belum\n",
        "if not os.path.exists(os.path.join('data','VOCtrainval_11-May-2012')):\n",
        "    with tarfile.open(os.path.join('data','VOCtrainval_11-May-2012.tar'), 'r') as tar:\n",
        "        tar.extractall('data')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **B. Memuat Gambar Target (Palettized Images)**"
      ],
      "metadata": {
        "id": "XwB3zHZ2VkT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 8.2: Konversi palettized image ke RGB\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def rgb_image_from_palette(image):\n",
        "    \"\"\"Mengembalikan RGB values dari PNG palettized image\"\"\"\n",
        "    palette = image.getpalette()\n",
        "    palette = np.array(palette).reshape(-1, 3)\n",
        "\n",
        "    if isinstance(image, Image.Image):\n",
        "        h, w = image.height, image.width\n",
        "        image = np.array(image).reshape(-1)\n",
        "\n",
        "    rgb_image = np.zeros(shape=(image.shape[0], 3))\n",
        "    rgb_image[(image != 0), :] = palette[image[(image != 0)], :]\n",
        "    rgb_image = rgb_image.reshape(h, w, 3)\n",
        "\n",
        "    return rgb_image"
      ],
      "metadata": {
        "id": "0QZLyINxVofD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **C. Pipeline Data Lengkap dengan ```tf.data```**"
      ],
      "metadata": {
        "id": "BvfB9YKJVqUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 8.6: Pipeline tf.data lengkap\n",
        "def get_subset_tf_dataset(\n",
        "    subset_filename_gen_func, batch_size, epochs,\n",
        "    input_size=(256, 256), output_size=None,\n",
        "    resize_to_before_crop=None, augmentation=False, shuffle=False\n",
        "):\n",
        "    # Generator filenames\n",
        "    filename_ds = tf.data.Dataset.from_generator(\n",
        "        subset_filename_gen_func, output_types=(tf.string, tf.string)\n",
        "    )\n",
        "\n",
        "    # Load images\n",
        "    def load_image_func(image):\n",
        "        img = np.array(Image.open(image))\n",
        "        return img\n",
        "\n",
        "    image_ds = filename_ds.map(lambda x, y: (\n",
        "        tf.image.decode_jpeg(tf.io.read_file(x)),\n",
        "        tf.numpy_function(load_image_func, [y], [tf.uint8])\n",
        "    )).cache()  # Optimization: cache di memory\n",
        "\n",
        "    # Normalisasi\n",
        "    image_ds = image_ds.map(lambda x, y: (tf.cast(x, 'float32')/255.0, y))\n",
        "\n",
        "    # Resize/Crop dengan augmentasi\n",
        "    def randomly_crop_or_resize(x, y):\n",
        "        # Fungsi untuk random crop dan resize\n",
        "        def rand_crop(x, y):\n",
        "            x = tf.image.resize(x, resize_to_before_crop, method='bilinear')\n",
        "            y = tf.cast(\n",
        "                tf.image.resize(\n",
        "                    tf.transpose(y, [1, 2, 0]),\n",
        "                    resize_to_before_crop, method='nearest'\n",
        "                ),\n",
        "                'float32'\n",
        "            )\n",
        "            # Random crop\n",
        "            offset_h = tf.random.uniform([], 0, x.shape[0]-input_size[0], dtype='int32')\n",
        "            offset_w = tf.random.uniform([], 0, x.shape[1]-input_size[1], dtype='int32')\n",
        "\n",
        "            x = tf.image.crop_to_bounding_box(\n",
        "                x, offset_h, offset_w, input_size[0], input_size[1]\n",
        "            )\n",
        "            y = tf.image.crop_to_bounding_box(\n",
        "                y, offset_h, offset_w, input_size[0], input_size[1]\n",
        "            )\n",
        "            return x, y\n",
        "\n",
        "        def resize(x, y):\n",
        "            x = tf.image.resize(x, input_size, method='bilinear')\n",
        "            y = tf.cast(\n",
        "                tf.image.resize(\n",
        "                    tf.transpose(y, [1, 2, 0]),\n",
        "                    input_size, method='nearest'\n",
        "                ),\n",
        "                'float32'\n",
        "            )\n",
        "            return x, y\n",
        "\n",
        "        if augmentation:\n",
        "            rand = tf.random.uniform([], 0.0, 1.0)\n",
        "            x, y = tf.cond(\n",
        "                rand < 0.5,\n",
        "                lambda: rand_crop(x, y),\n",
        "                lambda: resize(x, y)\n",
        "            )\n",
        "        else:\n",
        "            x, y = resize(x, y)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    image_ds = image_ds.map(randomly_crop_or_resize)\n",
        "\n",
        "    # Augmentasi tambahan\n",
        "    if augmentation:\n",
        "        # Random flip horizontal\n",
        "        def randomly_flip_horizontal(x, y):\n",
        "            rand = tf.random.uniform([], 0.0, 1.0)\n",
        "            def flip(x, y):\n",
        "                return tf.image.flip_left_right(x), tf.image.flip_left_right(y)\n",
        "            x, y = tf.cond(rand < 0.5, lambda: flip(x, y), lambda: (x, y))\n",
        "            return x, y\n",
        "\n",
        "        image_ds = image_ds.map(randomly_flip_horizontal)\n",
        "        image_ds = image_ds.map(lambda x, y: (tf.image.random_hue(x, 0.1), y))\n",
        "        image_ds = image_ds.map(lambda x, y: (tf.image.random_brightness(x, 0.1), y))\n",
        "        image_ds = image_ds.map(lambda x, y: (tf.image.random_contrast(x, 0.8, 1.2), y))\n",
        "\n",
        "    # Shuffle dan batch\n",
        "    if shuffle:\n",
        "        image_ds = image_ds.shuffle(buffer_size=batch_size*5)\n",
        "\n",
        "    image_ds = image_ds.batch(batch_size).repeat(epochs)\n",
        "    image_ds = image_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # Remove channel dimension dari target\n",
        "    image_ds = image_ds.map(lambda x, y: (x, tf.squeeze(y)))\n",
        "\n",
        "    return image_ds"
      ],
      "metadata": {
        "id": "mwBe7ofIVuKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **D. Optimasi Pipeline**\n",
        "* ```.cache()```: Menyimpan data di memory setelah load pertama\n",
        "* ```.prefetch()```: Prefetch data saat training berlangsung\n",
        "* ```.shuffle()```: Shuffle dengan buffer size optimal"
      ],
      "metadata": {
        "id": "ww76LSaSVw06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Model DeepLab v3**\n",
        "---\n",
        "### **A. Konsep Inti**\n",
        "* Backbone: ResNet-50 pretrained\n",
        "* Atrous Convolution: Convolution dengan \"holes\" untuk receptive field lebih besar tanpa tambahan parameter\n",
        "* ASPP (Atrous Spatial Pyramid Pooling): Menggabungkan informasi multi-scale\n",
        "\n",
        "### **B. Atrous Convolution**"
      ],
      "metadata": {
        "id": "iFEno40HV5pX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard vs Atrous Convolution\n",
        "# Standard 3x3 conv: receptive field 3x3\n",
        "# Atrous conv rate=2: receptive field 5x5\n",
        "# Atrous conv rate=3: receptive field 7x7"
      ],
      "metadata": {
        "id": "9F4zMnBfWG4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **C. Implementasi DeepLab v3**"
      ],
      "metadata": {
        "id": "q6u-OW77WIhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 8.8-8.12: Implementasi DeepLab v3 lengkap\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Block Level 3 (conv layer dengan batch norm)\n",
        "def block_level3(inp, filters, kernel_size, rate, block_id, convlayer_id, activation=True):\n",
        "    conv_name = f'conv5_block{block_id}_{convlayer_id}_conv'\n",
        "    bn_name = f'conv5_block{block_id}_{convlayer_id}_bn'\n",
        "\n",
        "    conv_out = layers.Conv2D(\n",
        "        filters, kernel_size, dilation_rate=rate,\n",
        "        padding='same', name=conv_name\n",
        "    )(inp)\n",
        "\n",
        "    bn_out = layers.BatchNormalization(name=bn_name)(conv_out)\n",
        "\n",
        "    if activation:\n",
        "        return layers.Activation('relu',\n",
        "            name=f'conv5_block{block_id}_{convlayer_id}_relu'\n",
        "        )(bn_out)\n",
        "    return bn_out\n",
        "\n",
        "# Block Level 2 (3 layer convolution)\n",
        "def block_level2(inp, rate, block_id):\n",
        "    block_1_out = block_level3(inp, 512, (1,1), rate, block_id, 1)\n",
        "    block_2_out = block_level3(block_1_out, 512, (3,3), rate, block_id, 2)\n",
        "    block_3_out = block_level3(\n",
        "        block_2_out, 2048, (1,1), rate, block_id, 3, activation=False\n",
        "    )\n",
        "    return block_3_out\n",
        "\n",
        "# ASPP Module\n",
        "def atrous_spatial_pyramid_pooling(inp):\n",
        "    # Part A: Multi-scale atrous convolutions\n",
        "    outa_1 = block_level3(inp, 256, (1,1), 1, '_aspp_a', 1, activation='relu')\n",
        "    outa_2 = block_level3(inp, 256, (3,3), 6, '_aspp_a', 2, activation='relu')\n",
        "    outa_3 = block_level3(inp, 256, (3,3), 12, '_aspp_a', 3, activation='relu')\n",
        "    outa_4 = block_level3(inp, 256, (3,3), 18, '_aspp_a', 4, activation='relu')\n",
        "\n",
        "    # Part B: Global context\n",
        "    outb_1_avg = layers.Lambda(\n",
        "        lambda x: tf.reduce_mean(x, axis=[1,2], keepdims=True)\n",
        "    )(inp)\n",
        "    outb_1_conv = block_level3(outb_1_avg, 256, (1,1), 1, '_aspp_b', 1, activation='relu')\n",
        "    outb_1_up = layers.UpSampling2D((24,24), interpolation='bilinear')(outb_1_conv)\n",
        "\n",
        "    # Concatenate semua output\n",
        "    out_aspp = layers.Concatenate()([outa_1, outa_2, outa_3, outa_4, outb_1_up])\n",
        "    return out_aspp\n",
        "\n",
        "# Build Model Lengkap\n",
        "def build_deeplabv3(input_size=(384, 384, 3), num_classes=21):\n",
        "    inputs = layers.Input(shape=input_size)\n",
        "\n",
        "    # Backbone ResNet50 sampai conv4\n",
        "    resnet50 = tf.keras.applications.ResNet50(\n",
        "        include_top=False, input_tensor=inputs, pooling=None\n",
        "    )\n",
        "\n",
        "    # Get output sampai conv4 block\n",
        "    for layer in resnet50.layers:\n",
        "        if layer.name == \"conv5_block1_1_conv\":\n",
        "            break\n",
        "        out = layer.output\n",
        "\n",
        "    resnet50_upto_conv4 = models.Model(resnet50.input, out)\n",
        "\n",
        "    # Conv5 block dengan atrous convolution\n",
        "    def resnet_block(inp, rate):\n",
        "        # Implementasi conv5 block dengan atrous conv\n",
        "        # ... (lihat listing 8.10)\n",
        "        return block_output\n",
        "\n",
        "    # ASPP\n",
        "    aspp_out = atrous_spatial_pyramid_pooling(resnet_block_out)\n",
        "\n",
        "    # Final layers\n",
        "    out = layers.Conv2D(num_classes, (1,1), padding='same')(aspp_out)\n",
        "    final_out = layers.UpSampling2D((16,16), interpolation='bilinear')(out)\n",
        "\n",
        "    model = models.Model(inputs, final_out)\n",
        "    return model"
      ],
      "metadata": {
        "id": "D5hrvErvWKWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Loss Functions dan Metrics untuk Segmentasi**\n",
        "---\n",
        "### **A Weighted Cross-Entropy Loss**"
      ],
      "metadata": {
        "id": "gNiMrfkwWM4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 8.13-8.14: Weighted CE Loss\n",
        "def get_label_weights(y_true, y_pred):\n",
        "    # Hitung class weights berdasarkan distribusi pixel\n",
        "    weights = tf.reduce_sum(tf.one_hot(y_true, num_classes), axis=[1,2])\n",
        "    tot = tf.reduce_sum(weights, axis=-1, keepdims=True)\n",
        "    weights = (tot - weights) / tot\n",
        "\n",
        "    y_true_flat = tf.reshape(y_true, [-1, y_pred.shape[1]*y_pred.shape[2]])\n",
        "    y_weights = tf.gather(params=weights, indices=y_true_flat, batch_dims=1)\n",
        "    return tf.reshape(y_weights, [-1])\n",
        "\n",
        "def ce_weighted_from_logits(num_classes):\n",
        "    def loss_fn(y_true, y_pred):\n",
        "        valid_mask = tf.cast(\n",
        "            tf.reshape((y_true <= num_classes - 1), [-1, 1]), 'int32'\n",
        "        )\n",
        "        y_true = tf.cast(y_true, 'int32')\n",
        "        y_true.set_shape([None, y_pred.shape[1], y_pred.shape[2]])\n",
        "\n",
        "        y_weights = get_label_weights(y_true, y_pred)\n",
        "        y_pred_unwrap = tf.reshape(y_pred, [-1, num_classes])\n",
        "        y_true_unwrap = tf.reshape(y_true, [-1])\n",
        "\n",
        "        return tf.reduce_mean(\n",
        "            y_weights * tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                y_true_unwrap * tf.squeeze(valid_mask),\n",
        "                y_pred_unwrap * tf.cast(valid_mask, 'float32')\n",
        "            )\n",
        "        )\n",
        "    return loss_fn"
      ],
      "metadata": {
        "id": "-W8mnJpgWRhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **B. Dice Loss**"
      ],
      "metadata": {
        "id": "IGUkxJNTWUOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 8.15: Dice Loss\n",
        "def dice_loss_from_logits(num_classes):\n",
        "    def loss_fn(y_true, y_pred):\n",
        "        smooth = 1.0\n",
        "        y_true = tf.cast(y_true, 'int32')\n",
        "        y_true.set_shape([None, y_pred.shape[1], y_pred.shape[2]])\n",
        "\n",
        "        y_weights = tf.reshape(get_label_weights(y_true, y_pred), [-1, 1])\n",
        "        y_pred = tf.nn.softmax(y_pred)\n",
        "\n",
        "        y_true_unwrap = tf.reshape(y_true, [-1])\n",
        "        y_true_onehot = tf.one_hot(tf.cast(y_true_unwrap, 'int32'), num_classes)\n",
        "        y_pred_unwrap = tf.reshape(y_pred, [-1, num_classes])\n",
        "\n",
        "        intersection = tf.reduce_sum(y_true_onehot * y_pred_unwrap * y_weights)\n",
        "        union = tf.reduce_sum((y_true_onehot + y_pred_unwrap) * y_weights)\n",
        "\n",
        "        score = (2. * intersection + smooth) / (union + smooth)\n",
        "        return 1 - score\n",
        "    return loss_fn"
      ],
      "metadata": {
        "id": "ZUMRQWr5WVvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **C. Combined Loss**"
      ],
      "metadata": {
        "id": "ZH1HYRymWYMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 8.16: Combined CE + Dice Loss\n",
        "def ce_dice_loss_from_logits(num_classes):\n",
        "    def loss_fn(y_true, y_pred):\n",
        "        ce_loss = ce_weighted_from_logits(num_classes)(y_true, y_pred)\n",
        "        dice_loss = dice_loss_from_logits(num_classes)(y_true, y_pred)\n",
        "        return ce_loss + dice_loss\n",
        "    return loss_fn"
      ],
      "metadata": {
        "id": "3SAQRY7tWbwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **D. Evaluation Metrics**"
      ],
      "metadata": {
        "id": "bt8NgNFMWdfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 8.17-8.19: Metrics untuk segmentasi\n",
        "\n",
        "# Pixel Accuracy\n",
        "class PixelAccuracyMetric(tf.keras.metrics.Accuracy):\n",
        "    def __init__(self, num_classes, name='pixel_accuracy', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true.set_shape([None, y_pred.shape[1], y_pred.shape[2]])\n",
        "        y_true = tf.reshape(y_true, [-1])\n",
        "        y_pred = tf.reshape(tf.argmax(y_pred, axis=-1), [-1])\n",
        "\n",
        "        valid_mask = tf.reshape((y_true <= self.num_classes - 1), [-1])\n",
        "        y_true = tf.boolean_mask(y_true, valid_mask)\n",
        "        y_pred = tf.boolean_mask(y_pred, valid_mask)\n",
        "\n",
        "        super().update_state(y_true, y_pred)\n",
        "\n",
        "# Mean Accuracy\n",
        "class MeanAccuracyMetric(tf.keras.metrics.Mean):\n",
        "    def __init__(self, num_classes, name='mean_accuracy', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        smooth = 1\n",
        "        y_true.set_shape([None, y_pred.shape[1], y_pred.shape[2]])\n",
        "        y_true = tf.reshape(y_true, [-1])\n",
        "        y_pred = tf.reshape(tf.argmax(y_pred, axis=-1), [-1])\n",
        "\n",
        "        valid_mask = tf.reshape((y_true <= self.num_classes - 1), [-1])\n",
        "        y_true = tf.boolean_mask(y_true, valid_mask)\n",
        "        y_pred = tf.boolean_mask(y_pred, valid_mask)\n",
        "\n",
        "        conf_matrix = tf.math.confusion_matrix(\n",
        "            y_true, y_pred, num_classes=self.num_classes\n",
        "        )\n",
        "        true_pos = tf.linalg.diag_part(conf_matrix)\n",
        "        mean_acc = tf.reduce_mean(\n",
        "            (true_pos + smooth) / (tf.reduce_sum(conf_matrix, axis=1) + smooth)\n",
        "        )\n",
        "        super().update_state(mean_acc)\n",
        "\n",
        "# Mean IoU\n",
        "class MeanIoUMetric(tf.keras.metrics.MeanIoU):\n",
        "    def __init__(self, num_classes, name='mean_iou', **kwargs):\n",
        "        super().__init__(num_classes=num_classes, name=name, **kwargs)\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true.set_shape([None, y_pred.shape[1], y_pred.shape[2]])\n",
        "        y_true = tf.reshape(y_true, [-1])\n",
        "        y_pred = tf.reshape(tf.argmax(y_pred, axis=-1), [-1])\n",
        "\n",
        "        valid_mask = tf.reshape((y_true <= self.num_classes - 1), [-1])\n",
        "        y_true = tf.boolean_mask(y_true, valid_mask)\n",
        "        y_pred = tf.boolean_mask(y_pred, valid_mask)\n",
        "\n",
        "        super().update_state(y_true, y_pred)"
      ],
      "metadata": {
        "id": "XmVm5tBxWfLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Training dan Evaluasi**\n",
        "---\n",
        "### **A. Compile Model**"
      ],
      "metadata": {
        "id": "weZCjgdKWhe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile dengan loss dan metrics kustom\n",
        "deeplabv3.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=ce_dice_loss_from_logits(num_classes=21),\n",
        "    metrics=[\n",
        "        MeanIoUMetric(num_classes=21),\n",
        "        MeanAccuracyMetric(num_classes=21),\n",
        "        PixelAccuracyMetric(num_classes=21)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "YHPB6YhsWlLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **B. Training dengan Callbacks**"
      ],
      "metadata": {
        "id": "HCg3bhudWmZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 8.20: Training model\n",
        "import os\n",
        "\n",
        "# Setup callbacks\n",
        "csv_logger = tf.keras.callbacks.CSVLogger(\n",
        "    os.path.join('eval', 'deeplabv3_training.log')\n",
        ")\n",
        "\n",
        "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.1, patience=3, min_lr=1e-8\n",
        ")\n",
        "\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=6, restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Training\n",
        "history = deeplabv3.fit(\n",
        "    x=train_ds,\n",
        "    steps_per_epoch=n_train_steps,\n",
        "    validation_data=val_ds,\n",
        "    validation_steps=n_val_steps,\n",
        "    epochs=25,\n",
        "    callbacks=[csv_logger, lr_callback, es_callback]\n",
        ")"
      ],
      "metadata": {
        "id": "pcFwQkmlWoBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **C. Evaluasi dan Visualisasi**"
      ],
      "metadata": {
        "id": "WYPYSqPSWp67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "test_results = deeplabv3.evaluate(test_ds, steps=n_test_steps)\n",
        "print(f\"Test Results - Loss: {test_results[0]:.4f}, \"\n",
        "      f\"Mean IoU: {test_results[1]:.4f}, \"\n",
        "      f\"Mean Accuracy: {test_results[2]:.4f}, \"\n",
        "      f\"Pixel Accuracy: {test_results[3]:.4f}\")\n",
        "\n",
        "# Visualize predictions\n",
        "def visualize_predictions(model, dataset, n_samples=5):\n",
        "    fig, axes = plt.subplots(n_samples, 3, figsize=(15, 5*n_samples))\n",
        "\n",
        "    for i, (image, mask) in enumerate(dataset.take(n_samples)):\n",
        "        pred = model.predict(tf.expand_dims(image, axis=0))\n",
        "        pred_mask = tf.argmax(pred[0], axis=-1)\n",
        "\n",
        "        axes[i, 0].imshow(image)\n",
        "        axes[i, 0].set_title(\"Original\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        axes[i, 1].imshow(mask)\n",
        "        axes[i, 1].set_title(\"Ground Truth\")\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        axes[i, 2].imshow(pred_mask)\n",
        "        axes[i, 2].set_title(\"Prediction\")\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "IUuzSNG6WtOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Key Takeaways**\n",
        "---\n",
        "* tf.data pipeline penting untuk efisiensi I/O\n",
        "* Atrous convolution meningkatkan receptive field tanpa tambahan parameter\n",
        "* ASPP mengatasi masalah multi-scale information\n",
        "* Combined loss (CE + Dice) bekerja baik untuk class imbalance\n",
        "* Custom metrics diperlukan untuk evaluasi segmentasi yang akurat"
      ],
      "metadata": {
        "id": "Y2Bew8CYWsF4"
      }
    }
  ]
}