{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAf31oMNMwF3xDx3WdRlMv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaraRahma536/TensorFlow-in-Action/blob/main/Chapter_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapter 11: Sequence-to-Sequence Learning**"
      ],
      "metadata": {
        "id": "X5NdyDYGb2ob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Memahami Data Terjemahan Mesin**\n",
        "---\n",
        "Pada bagian ini, kita mempersiapkan dataset terjemahan Inggris-Jerman untuk model sequence-to-sequence (seq2seq). Dataset diambil dari http://www.manythings.org/anki/deu-eng.zip, yang berisi frasa paralel Inggris-Jerman.\n",
        "\n",
        "Proses Persiapan Data:\n",
        "* Memuat Data: Dataset dimuat ke dalam DataFrame pandas dengan kolom \"EN\" (Inggris) dan \"DE\" (Jerman).\n",
        "* Pembersihan: Menghapus baris dengan karakter Unicode yang bermasalah.\n",
        "* Penambahan Token Khusus: Menambahkan token sos (start of sentence) di awal dan eos (end of sentence) di akhir setiap terjemahan Jerman.\n",
        "* Pembagian Data: Data dibagi menjadi 80% train, 10% validation, dan 10% test.\n",
        "* Analisis Statistik: Menganalisis ukuran kosakata dan panjang sekuens untuk kedua bahasa.\n",
        "\n",
        "Code Utama:"
      ],
      "metadata": {
        "id": "zFRbZQAgb4PG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aij2JL59a-nr"
      },
      "outputs": [],
      "source": [
        "# Memuat data\n",
        "df = pd.read_csv('data/deu.txt', delimiter='\\t', header=None)\n",
        "df.columns = [\"EN\", \"DE\", \"Attribution\"]\n",
        "df = df[[\"EN\", \"DE\"]]\n",
        "\n",
        "# Menambahkan token sos dan eos\n",
        "df[\"DE\"] = 'sos ' + df[\"DE\"] + ' eos'\n",
        "\n",
        "# Membagi data\n",
        "train_df, valid_df, test_df = split_data(df)\n",
        "\n",
        "# Analisis kosakata\n",
        "en_vocab = get_vocabulary_size_greater_than(en_words, n=10)\n",
        "de_vocab = get_vocabulary_size_greater_than(de_words, n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Membangun Model Penerjemah Seq2Seq Inggris-Jerman**\n",
        "---\n",
        "Model seq2seq terdiri dari encoder dan decoder. Encoder memproses input bahasa sumber (Inggris) dan menghasilkan vektor konteks, yang kemudian digunakan decoder untuk menghasilkan terjemahan (Jerman).\n",
        "\n",
        "Komponen Utama:\n",
        "* TextVectorization Layer: Layer Keras yang mengubah string menjadi token IDs.\n",
        "* Encoder: Menggunakan Bidirectional GRU untuk memahami konteks dari input Inggris.\n",
        "* Decoder: Menggunakan GRU (non-bidirectional) dan dense layers untuk menghasilkan terjemahan kata per kata.\n",
        "\n",
        "Code Pembuatan Model:"
      ],
      "metadata": {
        "id": "uDHBzqP0cJRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mendefinisikan TextVectorization\n",
        "en_vectorizer = TextVectorization(max_tokens=en_vocab+2, output_mode='int')\n",
        "en_vectorizer.adapt(train_df[\"EN\"])\n",
        "\n",
        "# Mendefinisikan Encoder\n",
        "encoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(1,), dtype=tf.string),\n",
        "    en_vectorizer,\n",
        "    tf.keras.layers.Embedding(en_vocab+2, 128, mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128))\n",
        "])\n",
        "\n",
        "# Mendefinisikan Decoder\n",
        "decoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(1,), dtype=tf.string),\n",
        "    de_vectorizer,\n",
        "    tf.keras.layers.Embedding(de_vocab+2, 128, mask_zero=True),\n",
        "    tf.keras.layers.GRU(256, return_sequences=True),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(de_vocab+2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Menggabungkan Encoder-Decoder\n",
        "context_vector = encoder(english_input)\n",
        "translation_output = decoder(german_input, initial_state=context_vector)"
      ],
      "metadata": {
        "id": "ARIXZNGfcPqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Melatih dan Mengevaluasi Model**\n",
        "---\n",
        "Model dilatih menggunakan teacher forcing, di mana decoder menerima input dari terjemahan sebenarnya (bukan prediksinya sendiri) selama pelatihan. Evaluasi menggunakan akurasi kata-per-kata dan skor BLEU.\n",
        "\n",
        "Proses Pelatihan:\n",
        "* Teacher Forcing: Decoder dilatih untuk memprediksi kata berikutnya berdasarkan kata sebelumnya yang sebenarnya.\n",
        "* Custom Training Loop: Dilakukan karena memerlukan perhitungan BLEU yang tidak tersedia di Keras.\n",
        "* Evaluasi: Dilakukan pada data validasi dan test menggunakan loss, akurasi, dan BLEU.\n",
        "\n",
        "Code Pelatihan dan Evaluasi:"
      ],
      "metadata": {
        "id": "-rMDkHAucR61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi evaluasi dengan BLEU\n",
        "def evaluate_model(model, data, batch_size):\n",
        "    bleu_scores = []\n",
        "    for batch in data:\n",
        "        pred = model.predict(batch)\n",
        "        bleu = calculate_bleu(batch['target'], pred)\n",
        "        bleu_scores.append(bleu)\n",
        "    return np.mean(bleu_scores)\n",
        "\n",
        "# Pelatihan dengan custom loop\n",
        "for epoch in range(epochs):\n",
        "    for batch in train_batches:\n",
        "        model.train_on_batch(batch['input'], batch['target'])\n",
        "    val_bleu = evaluate_model(model, val_data, batch_size)\n",
        "    print(f\"Epoch {epoch+1}, BLEU: {val_bleu}\")"
      ],
      "metadata": {
        "id": "nH0aA2zUcWJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Dari Pelatihan ke Inferensi: Mendefinisikan Model Inferensi\n",
        "Model yang dilatih dengan teacher forcing tidak dapat langsung digunakan untuk inferensi karena memerlukan input bahasa target (yang belum diketahui). Oleh karena itu, kita membuat model inferensi rekursif di mana decoder menggunakan prediksinya sendiri sebagai input untuk langkah berikutnya.\n",
        "\n",
        "Perubahan pada Inferensi:\n",
        "* Decoder Rekursif: Menggunakan output state GRU dari langkah sebelumnya sebagai initial state untuk langkah berikutnya.\n",
        "* Stop Condition: Berhenti ketika decoder memprediksi token eos.\n",
        "\n",
        "Code Model Inferensi:"
      ],
      "metadata": {
        "id": "NBkoewVjcYTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat model inferensi\n",
        "def get_inference_model(trained_model):\n",
        "    encoder = trained_model.get_layer(\"encoder\")\n",
        "\n",
        "    # Decoder untuk inferensi\n",
        "    decoder_input = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
        "    state_input = tf.keras.Input(shape=(256,))\n",
        "\n",
        "    # Menggunakan layer dari model yang sudah dilatih\n",
        "    vectorized = de_vectorizer(decoder_input)\n",
        "    embedded = embedding_layer(vectorized)\n",
        "    gru_out = gru_layer(embedded, initial_state=state_input)\n",
        "    final_out = dense_layer(gru_out)\n",
        "\n",
        "    decoder = tf.keras.Model(\n",
        "        inputs=[decoder_input, state_input],\n",
        "        outputs=[final_out, gru_out]\n",
        "    )\n",
        "    return encoder, decoder\n",
        "\n",
        "# Fungsi menghasilkan terjemahan\n",
        "def translate(sentence, encoder, decoder):\n",
        "    context_vector = encoder.predict([sentence])\n",
        "    current_word = \"sos\"\n",
        "    translation = []\n",
        "\n",
        "    while current_word != \"eos\":\n",
        "        pred, state = decoder.predict([[current_word], context_vector])\n",
        "        current_word = de_vocabulary[np.argmax(pred)]\n",
        "        translation.append(current_word)\n",
        "        context_vector = state\n",
        "\n",
        "    return \" \".join(translation[:-1])  # Hapus 'eos'"
      ],
      "metadata": {
        "id": "1p8htKafcczW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Kesimpulan Utama**\n",
        "---\n",
        "* Model Seq2Seq cocok untuk tugas penerjemahan mesin karena dapat menangani input dan output dengan panjang variabel.\n",
        "* Teacher Forcing mempercepat pelatihan dengan memberikan target sebenarnya sebagai input decoder.\n",
        "* BLEU Score lebih baik daripada akurasi kata-per-kata untuk mengevaluasi kualitas terjemahan.\n",
        "* Model Inferensi memerlukan modifikasi khusus karena decoder harus bekerja secara rekursif menggunakan prediksinya sendiri."
      ],
      "metadata": {
        "id": "QIvHkinYcdZT"
      }
    }
  ]
}