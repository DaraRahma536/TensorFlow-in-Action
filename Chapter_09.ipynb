{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQv0euWbXUbYmH6dbFZauc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaraRahma536/TensorFlow-in-Action/blob/main/Chapter_09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapter 9: Natural Language Processing with TensorFlow – Sentiment Analysis**\n",
        "Bab ini membahas analisis sentimen menggunakan TensorFlow, dimulai dari preprocessing teks hingga implementasi model LSTM dengan word embeddings. Dataset yang digunakan adalah ulasan video game dari Amazon, dengan tujuan mengklasifikasikan ulasan menjadi positif (label 1) atau negatif (label 0).\n",
        "\n"
      ],
      "metadata": {
        "id": "piR4sy6WXoFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Langkah-langkah Utama:**\n",
        "# **1. Eksplorasi dan Preprocessing Data**\n",
        "---\n",
        "* Dataset: Video_Games_5.json.gz dari Amazon.\n",
        "* Preprocessing meliputi:\n",
        "* Mengonversi teks menjadi huruf kecil.\n",
        "* Menghapus tanda baca, angka, dan stopwords (kecuali \"not\" dan \"no\").\n",
        "* Lemmatization menggunakan WordNetLemmatizer dari NLTK.\n",
        "* Tokenization menjadi kata-kata individual.\n",
        "\n",
        "# **2. Persiapan Data untuk Model**\n",
        "---\n",
        "* Split Data: Training, validation, test dengan stratified sampling untuk menangani ketidakseimbangan kelas.\n",
        "* Analisis Vocabulary: Menghitung frekuensi kata dan menentukan ukuran vocabulary (n_vocab ≈ 11.800).\n",
        "* Analisis Panjang Sequence: Membagi ulasan menjadi kategori pendek (<5 kata), sedang (5–15 kata), dan panjang (>15 kata).\n",
        "* Tokenization dengan Keras: Mengonversi kata menjadi ID numerik menggunakan Tokenizer.\n",
        "\n",
        "# **3. Pipeline Data dengan TensorFlow**\n",
        "---\n",
        "* Bucket_by_sequence_length: Mengelompokkan ulasan dengan panjang seragam untuk efisiensi padding.\n",
        "* Menggunakan tf.RaggedTensor untuk menangani sequence dengan panjang variabel.\n",
        "* Pipeline akhir menghasilkan batch data siap untuk training.\n",
        "\n"
      ],
      "metadata": {
        "id": "YD3p8jhmXs5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Model LSTM untuk Analisis Sentimen**\n",
        "---\n",
        "Arsitektur Model:\n",
        "* Masking Layer untuk mengabaikan padding.\n",
        "* One-hot Encoding (dalam model awal) atau Embedding Layer (dalam model final).\n",
        "* LSTM Layer dengan 128 unit.\n",
        "* Dense Layer dengan 512 unit + ReLU.\n",
        "* Dropout Layer (0.5).\n",
        "* Output Layer dengan 1 unit + sigmoid.\n",
        "\n",
        "# **5. Training dan Evaluasi**\n",
        "---\n",
        "* Loss: binary_crossentropy.\n",
        "* Optimizer: Adam.\n",
        "* Class Weight: Memberi bobot lebih pada kelas minoritas (ulasan negatif).\n",
        "* Callback: CSVLogger, ReduceLROnPlateau, EarlyStopping.\n",
        "* Hasil Akhir: Akurasi ≈ 80–81% pada test set.\n",
        "\n",
        "# **6. Peningkatan dengan Word Embeddings**\n",
        "---\n",
        "* Mengganti one-hot encoding dengan Embedding Layer (tf.keras.layers.Embedding).\n",
        "* Embedding Layer belajar representasi vektor kata selama training.\n",
        "* Hasil: Peningkatan akurasi menjadi ≈ 81.1% pada test set.\n",
        "\n",
        "# **7. Validasi Manual Prediksi**\n",
        "---\n",
        "* Melihat ulasan paling negatif dan paling positif berdasarkan prediksi model.\n",
        "* Kesimpulan: Model memberikan prediksi yang sesuai dengan konteks ulasan."
      ],
      "metadata": {
        "id": "dm0q8ljaYNK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Kode Utama yang Direproduksi:**\n",
        "---\n",
        "### **1. Preprocessing Teks**"
      ],
      "metadata": {
        "id": "JACtC1QxYf77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(doc):\n",
        "    doc = doc.lower()\n",
        "    doc = doc.replace(\"n\\'t \", ' not ')\n",
        "    doc = re.sub(r\"(?:\\\\'ll |\\\\'re |\\\\'d |\\\\'ve )\", \" \", doc)\n",
        "    doc = re.sub(r\"/d+\", \"\", doc)\n",
        "    tokens = [w for w in word_tokenize(doc) if w not in EN_STOPWORDS and w not in string.punctuation]\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    clean_text = [lemmatizer.lemmatize(w, pos=p[0].lower()) if p[0]=='N' or p[0]=='V' else w for (w, p) in pos_tags]\n",
        "    return clean_text"
      ],
      "metadata": {
        "id": "adqHyGQmXlZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Tokenization dengan Keras**"
      ],
      "metadata": {
        "id": "o6dPNTNKYl_6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch020GY_XiIZ"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False, filters='', split=' ', char_level=False)\n",
        "tokenizer.fit_on_texts(tr_x.tolist())\n",
        "tr_x = tokenizer.texts_to_sequences(tr_x.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Pipeline Data dengan Bucketing**"
      ],
      "metadata": {
        "id": "yCBeDPfhYoFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tf_pipeline(text_seq, labels, batch_size=64, bucket_boundaries=[5,15], max_length=50, shuffle=False):\n",
        "    data_seq = [[b]+a for a,b in zip(text_seq, labels)]\n",
        "    tf_data = tf.ragged.constant(data_seq)[:, :max_length]\n",
        "    text_ds = tf.data.Dataset.from_tensor_slices(tf_data)\n",
        "    bucket_fn = tf.data.experimental.bucket_by_sequence_length(\n",
        "        lambda x: tf.cast(tf.shape(x)[0], 'int32'),\n",
        "        bucket_boundaries=bucket_boundaries,\n",
        "        bucket_batch_sizes=[batch_size]*3,\n",
        "        padding_values=0,\n",
        "        pad_to_bucket_boundary=False\n",
        "    )\n",
        "    text_ds = text_ds.map(lambda x: x).apply(bucket_fn)\n",
        "    if shuffle:\n",
        "        text_ds = text_ds.shuffle(buffer_size=10*batch_size)\n",
        "    text_ds = text_ds.map(lambda x: (x[:,1:], x[:,0]))\n",
        "    return text_ds"
      ],
      "metadata": {
        "id": "WyPcJnT6YpwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Model LSTM dengan Embedding**"
      ],
      "metadata": {
        "id": "iV7FUoKvYr3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=n_vocab, output_dim=128, mask_zero=True, input_shape=(None,)),\n",
        "    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "oa1Mxl9VZDT_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}