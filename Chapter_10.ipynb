{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMt/xUpB690d7ydlXWU4v1T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaraRahma536/TensorFlow-in-Action/blob/main/Chapter_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapter 10: Natural Language Processing with TensorFlow – Language Modeling**\n",
        "Bab ini membahas pemodelan bahasa (language modeling) menggunakan TensorFlow dengan model GRU (Gated Recurrent Unit). Tujuannya adalah untuk memprediksi kata berikutnya berdasarkan urutan kata sebelumnya, yang kemudian dapat digunakan untuk membuat teks baru (misalnya, cerita anak-anak). Dataset yang digunakan adalah bAbI dataset dari Facebook, berisi cerita anak-anak."
      ],
      "metadata": {
        "id": "rSzEO-jeZNSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Langkah-langkah Utama:**\n",
        "# **1. Memproses Data**\n",
        "---\n",
        "* Dataset: CBTest.tgz dari bAbI dataset.\n",
        "* Preprocessing:\n",
        "-&nbsp;Membaca cerita dari file teks.  \n",
        "-&nbsp;Menggunakan n-grams (bigrams) untuk mengurangi ukuran vocabulary.  \n",
        "-&nbsp;Tokenization dengan Tokenizer dari Keras.  \n",
        "* Pipeline Data dengan TensorFlow:\n",
        "-&nbsp;Menggunakan tf.data.Dataset.window() untuk membuat window dari urutan teks.\n",
        "-&nbsp;Memisahkan input dan target (target adalah input yang digeser satu posisi ke kanan).\n",
        "\n",
        "# **2. Model GRU untuk Language Modeling**\n",
        "---\n",
        "Arsitektur Model:\n",
        "* Embedding Layer untuk merepresentasikan n-grams sebagai vektor.\n",
        "* GRU Layer dengan 1024 unit.\n",
        "* Dense Layer dengan 512 unit + ReLU.\n",
        "* Dense Layer akhir dengan ukuran vocabulary + softmax.\n",
        "\n",
        "GRU vs LSTM: GRU lebih sederhana dengan dua gate (reset dan update) dan satu state, namun performa setara dengan LSTM.\n",
        "\n",
        "# **3. Metrik Evaluasi: Perplexity**\n",
        "---\n",
        "* Perplexity mengukur seberapa \"terkejut\" model melihat kata target diberikan urutan sebelumnya.\n",
        "* Perhitungan: Perplexity = exp(mean(cross_entropy_loss)).\n",
        "* Implementasi: Membuat custom metric PerplexityMetric dengan subclass tf.keras.metrics.Mean.\n",
        "\n",
        "# **4. Training dan Evaluasi Model**\n",
        "---\n",
        "* Training dengan callbacks: CSVLogger, ReduceLROnPlateau, EarlyStopping.\n",
        "* Hasil: Perplexity validasi ≈ 9.5, akurasi ≈ 45.7% pada test set.\n",
        "* Penyimpanan model: Model disimpan dalam format .h5.\n",
        "\n",
        "# **5. Generasi Teks dengan Greedy Decoding**\n",
        "---\n",
        "* Inference Model: Membuat model khusus untuk generasi teks dengan functional API.\n",
        "* Proses Greedy Decoding:\n",
        "-&nbsp;Mulai dengan seed teks.\n",
        "-&nbsp;Prediksi kata berikutnya secara rekursif, menggunakan kata yang diprediksi sebagai input berikutnya.\n",
        "* Hasil: Teks yang dihasilkan cukup baik tetapi terdapat kesalahan tata bahasa dan ejaan.\n",
        "\n",
        "# **6. Peningkatan dengan Beam Search**\n",
        "---\n",
        "* Beam Search: Memprediksi beberapa langkah ke depan dan memilih urutan dengan probabilitas gabungan tertinggi.\n",
        "* Parameter: beam_width (jumlah kandidat per langkah) dan beam_depth (jumlah langkah pencarian).\n",
        "* Implementasi: Fungsi rekursif untuk menjelajahi ruang pencarian.\n",
        "* Hasil: Teks yang dihasilkan lebih koheren dan gramatikal dibanding greedy decoding."
      ],
      "metadata": {
        "id": "UcDqYA5oZiix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Kode Utama yang Direproduksi:**\n",
        "---\n",
        "### **1. Preprocessing dan Tokenization**"
      ],
      "metadata": {
        "id": "N1ZgTqPyaRWW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GVWDffiXq3e"
      },
      "outputs": [],
      "source": [
        "def get_ngrams(text, n):\n",
        "    return [text[i:i+n] for i in range(0, len(text), n)]\n",
        "\n",
        "train_ngram_stories = [get_ngrams(s, ngrams) for s in stories]\n",
        "tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)\n",
        "tokenizer.fit_on_texts(train_ngram_stories)\n",
        "train_data_seq = tokenizer.texts_to_sequences(train_ngram_stories)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Pipeline Data dengan Window**"
      ],
      "metadata": {
        "id": "7HkSBJVoaVLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tf_pipeline(data_seq, n_seq, batch_size=64, shift=1, shuffle=True):\n",
        "    text_ds = tf.data.Dataset.from_tensor_slices(tf.ragged.constant(data_seq))\n",
        "    if shuffle:\n",
        "        text_ds = text_ds.shuffle(buffer_size=len(data_seq)//2)\n",
        "\n",
        "    text_ds = text_ds.flat_map(\n",
        "        lambda x: tf.data.Dataset.from_tensor_slices(x)\n",
        "        .window(n_seq+1, shift=shift)\n",
        "        .flat_map(lambda window: window.batch(n_seq+1, drop_remainder=True))\n",
        "    )\n",
        "\n",
        "    if shuffle:\n",
        "        text_ds = text_ds.shuffle(buffer_size=10*batch_size)\n",
        "\n",
        "    text_ds = text_ds.batch(batch_size)\n",
        "    text_ds = tf.data.Dataset.zip(\n",
        "        text_ds.map(lambda x: (x[:,:-1], x[:, 1:]))\n",
        "    ).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return text_ds"
      ],
      "metadata": {
        "id": "Qkrq_ouuaXzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Model GRU untuk Language Modeling**"
      ],
      "metadata": {
        "id": "nBc7gs52aXkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=n_vocab+1, output_dim=512, input_shape=(None,)),\n",
        "    tf.keras.layers.GRU(1024, return_state=False, return_sequences=True),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(n_vocab, name='final_out'),\n",
        "    tf.keras.layers.Activation(activation='softmax')\n",
        "])\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy', PerplexityMetric()]\n",
        ")"
      ],
      "metadata": {
        "id": "yOP-L1kBaaS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Custom Metric: Perplexity**"
      ],
      "metadata": {
        "id": "8wjXVlxjadNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PerplexityMetric(tf.keras.metrics.Mean):\n",
        "    def __init__(self, name='perplexity', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=False, reduction='none'\n",
        "        )\n",
        "\n",
        "    def _calculate_perplexity(self, real, pred):\n",
        "        loss_ = self.cross_entropy(real, pred)\n",
        "        mean_loss = K.mean(loss_, axis=-1)\n",
        "        perplexity = K.exp(mean_loss)\n",
        "        return perplexity\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        perplexity = self._calculate_perplexity(y_true, y_pred)\n",
        "        super().update_state(perplexity)"
      ],
      "metadata": {
        "id": "0salL0UYaehJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Inference Model untuk Generasi Teks**"
      ],
      "metadata": {
        "id": "K1X8aQN4afqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp = tf.keras.layers.Input(shape=(None,))\n",
        "inp_state = tf.keras.layers.Input(shape=(1024,))\n",
        "\n",
        "emb_layer = tf.keras.layers.Embedding(input_dim=n_vocab+1, output_dim=512)\n",
        "emb_out = emb_layer(inp)\n",
        "\n",
        "gru_layer = tf.keras.layers.GRU(1024, return_state=True, return_sequences=True)\n",
        "gru_out, gru_state = gru_layer(emb_out, initial_state=inp_state)\n",
        "\n",
        "dense_layer = tf.keras.layers.Dense(512, activation='relu')\n",
        "dense_out = dense_layer(gru_out)\n",
        "\n",
        "final_layer = tf.keras.layers.Dense(n_vocab)\n",
        "final_out = final_layer(dense_out)\n",
        "softmax_out = tf.keras.layers.Activation(activation='softmax')(final_out)\n",
        "\n",
        "infer_model = tf.keras.models.Model(\n",
        "    inputs=[inp, inp_state], outputs=[softmax_out, gru_state]\n",
        ")\n",
        "\n",
        "# Transfer weights from trained model\n",
        "emb_layer.set_weights(model.get_layer('embedding').get_weights())\n",
        "gru_layer.set_weights(model.get_layer('gru').get_weights())\n",
        "dense_layer.set_weights(model.get_layer('dense').get_weights())\n",
        "final_layer.set_weights(model.get_layer('final_out').get_weights())"
      ],
      "metadata": {
        "id": "r4shaiPQaiI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Beam Search untuk Generasi Teks yang Lebih Baik**"
      ],
      "metadata": {
        "id": "oGTlaAmGahKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(model, input_, state, beam_depth=5, beam_width=3):\n",
        "    results = []\n",
        "\n",
        "    def recursive_fn(input_, state, sequence, log_prob, i):\n",
        "        if i == beam_depth:\n",
        "            results.append((list(sequence), state, np.exp(log_prob)))\n",
        "            return\n",
        "        else:\n",
        "            output, new_state = model.predict([input_, state])\n",
        "            top_probs, top_ids = tf.nn.top_k(output, k=beam_width)\n",
        "            top_probs, top_ids = top_probs.numpy().ravel(), top_ids.numpy().ravel()\n",
        "\n",
        "            for p, wid in zip(top_probs, top_ids):\n",
        "                new_log_prob = log_prob + np.log(p)\n",
        "                if len(sequence) > 0 and wid == sequence[-1]:\n",
        "                    new_log_prob += np.log(1e-1)\n",
        "                sequence.append(wid)\n",
        "                recursive_fn(np.array([[wid]]), new_state, sequence, new_log_prob, i+1)\n",
        "                sequence.pop()\n",
        "\n",
        "    recursive_fn(input_, state, [], 0.0, 0)\n",
        "    results = sorted(results, key=lambda x: x[2], reverse=True)\n",
        "    return results"
      ],
      "metadata": {
        "id": "vl9QCsXNamTX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}