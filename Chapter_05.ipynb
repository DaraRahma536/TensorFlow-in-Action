{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrvG+kXIWDv1NZYZHyDs36",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaraRahma536/TensorFlow-in-Action/blob/main/Chapter_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapter 5: State-of-the-Art in Deep Learning: Transformers**"
      ],
      "metadata": {
        "id": "P17z9u-yOxwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Representasi Teks sebagai Angka**\n",
        "---\n",
        "### **A. Masalah Dasar**\n",
        "Model deep learning memproses data numerik, tetapi teks bersifat kategorikal. Solusi: mengubah kata menjadi vektor.\n",
        "\n",
        "### **B. Proses Konversi**\n",
        "* Tokenisasi: Pecah kalimat menjadi kata/unit\n",
        "* Pembuatan Vocabulary: Buat kamus kata → ID unik"
      ],
      "metadata": {
        "id": "Map6zI8cO0CP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "I → 1, went → 2, to → 3, the → 4, beach → 5"
      ],
      "metadata": {
        "id": "V8EvZ50tO7Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Padding & Truncating: Samakan panjang kalimat\n",
        "-&nbsp;Padding: Tambah token <PAD> (ID 0) untuk kalimat pendek\n",
        "-&nbsp;Truncating: Potong kalimat panjang\n",
        "* One-Hot Encoding: Setiap ID diubah menjadi vektor biner\n"
      ],
      "metadata": {
        "id": "s7L2UTGLO702"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4Jf2j7ROk6S"
      },
      "outputs": [],
      "source": [
        "ID 1 → [0, 1, 0, 0, 0, ...]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **C. Kelemahan One-Hot Encoding**\n",
        "* Dimensi sangat besar (vocab size)\n",
        "* Tidak ada hubungan semantik antar kata\n",
        "* Solusi: Word Embeddings (seperti Word2Vec, GloVe) yang memetakan kata ke vektor padat (dense)"
      ],
      "metadata": {
        "id": "5P_TJ30mPIYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Arsitektur Transformer**\n",
        "---\n",
        "### **A. Encoder-Decoder Architecture**\n",
        "Transformer menggunakan pola encoder-decoder:\n",
        "* Encoder: Memetakan input (misal: kalimat Inggris) ke representasi laten\n",
        "* Decoder: Menggunakan representasi laten untuk menghasilkan output (misal: terjemahan Prancis)\n",
        "\n",
        "**Analogi**: Penerjemah manusia:\n",
        "* Dengarkan kalimat Prancis (encoder)\n",
        "* Pahami makna (representasi laten)\n",
        "* Terjemahkan ke Inggris (decoder)\n",
        "\n",
        "### **B. Komponen Encoder Layer**\n",
        "Setiap encoder layer terdiri dari:\n",
        "* Self-Attention Sublayer\n",
        "* Fully Connected Sublayer\n",
        "\n",
        "**Self-Attention Layer**\n",
        "Fungsi: Memungkinkan model \"melihat\" semua kata dalam kalimat sekaligus saat memproses satu kata.\n",
        "\n",
        "**Perbandingan dengan RNN:**\n",
        "* RNN: Proses kata per kata, bisa lupa kata awal\n",
        "* Self-Attention: Akses semua kata secara paralel\n",
        "\n",
        "Komputasi Self-Attention:"
      ],
      "metadata": {
        "id": "HCC_YZx2PORY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Input: X (n_words × d_model)\n",
        "Q = X * Wq  (Query)\n",
        "K = X * Wk  (Key)\n",
        "V = X * Wv  (Value)\n",
        "\n",
        "Attention = softmax((Q * K^T) / √d_k) * V"
      ],
      "metadata": {
        "id": "hhFe0ZG5Pkxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiga Komponen Kunci:**\n",
        "* **Query**: Kata yang sedang diproses\n",
        "* **Key**: Kata-kata kandidat untuk diperhatikan\n",
        "* **Value**: Representasi yang akan dijumlahkan secara terbobot\n",
        "\n",
        "### **C. Multi-Head Attention**\n",
        "* Daripada satu attention head, gunakan beberapa head (biasanya 8)\n",
        "* Setiap head belajar pola berbeda\n",
        "* Output semua head digabung (concatenate)"
      ],
      "metadata": {
        "id": "kjITLdJ9PmhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_head = d_model / n_heads"
      ],
      "metadata": {
        "id": "06DBXDoQPuDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **D. Masked Self-Attention (Decoder)**\n",
        "* Mencegah decoder \"mencontek\" kata masa depan\n",
        "* Membuat matriks lower-triangular dengan memberi nilai negatif besar ke posisi masa depan\n",
        "* Esensial untuk training yang benar\n",
        "\n",
        "### **E. Fully Connected Sublayer**\n",
        "Dense layer sederhana dengan ReLU"
      ],
      "metadata": {
        "id": "avYWbCHUPuZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h1 = ReLU(X * W1 + b1)\n",
        "h2 = h1 * W2 + b2  (tanpa aktivasi)"
      ],
      "metadata": {
        "id": "7nexFL1hP0Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Implementasi dengan TensorFlow/Keras**\n",
        "---\n",
        "### **A. SelfAttentionLayer (Custom Layer)**"
      ],
      "metadata": {
        "id": "lgzjMn_mP4K1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionLayer(layers.Layer):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.Wq = self.add_weight(...)\n",
        "        self.Wk = self.add_weight(...)\n",
        "        self.Wv = self.add_weight(...)\n",
        "\n",
        "    def call(self, q_x, k_x, v_x, mask=None):\n",
        "        q = tf.matmul(q_x, self.Wq)\n",
        "        k = tf.matmul(k_x, self.Wk)\n",
        "        v = tf.matmul(v_x, self.Wv)\n",
        "\n",
        "        # Masking untuk decoder\n",
        "        if mask is not None:\n",
        "            scores += mask * -1e9\n",
        "\n",
        "        attention = tf.nn.softmax(scores)\n",
        "        output = tf.matmul(attention, v)\n",
        "        return output"
      ],
      "metadata": {
        "id": "41YK1zvDP68q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **B. EncoderLayer**"
      ],
      "metadata": {
        "id": "2rx92FWHP87B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    def __init__(self, d, n_heads):\n",
        "        super().__init__()\n",
        "        self.attention_heads = [SelfAttentionLayer(d//n_heads) for _ in range(n_heads)]\n",
        "        self.fc_layer = FCLayer(2048, d)\n",
        "\n",
        "    def call(self, x):\n",
        "        # Multi-head attention\n",
        "        head_outputs = [head(x, x, x) for head in self.attention_heads]\n",
        "        concat = tf.concat(head_outputs, axis=-1)\n",
        "        # Fully connected\n",
        "        output = self.fc_layer(concat)\n",
        "        return output"
      ],
      "metadata": {
        "id": "2VZgxCFyP97d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **C. DecoderLayer**"
      ],
      "metadata": {
        "id": "QFRpzUUwP_kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "    def __init__(self, d, n_heads):\n",
        "        super().__init__()\n",
        "        self.masked_attention = [SelfAttentionLayer(d//n_heads) for _ in range(n_heads)]\n",
        "        self.encoder_decoder_attention = [SelfAttentionLayer(d//n_heads) for _ in range(n_heads)]\n",
        "        self.fc_layer = FCLayer(2048, d)\n",
        "\n",
        "    def call(self, decoder_input, encoder_output, mask):\n",
        "        # Masked self-attention\n",
        "        masked_out = multi_head_attention(decoder_input, decoder_input, decoder_input, mask)\n",
        "        # Encoder-decoder attention\n",
        "        attn_out = multi_head_attention(masked_out, encoder_output, encoder_output)\n",
        "        # Fully connected\n",
        "        output = self.fc_layer(attn_out)\n",
        "        return output"
      ],
      "metadata": {
        "id": "Au8d2ijVQA3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **D. Model Transformer Lengkap**"
      ],
      "metadata": {
        "id": "oysXTOJuQCnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "n_steps = 25        # Max sentence length\n",
        "n_en_vocab = 300    # English vocabulary size\n",
        "n_de_vocab = 400    # French vocabulary size\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "\n",
        "# Encoder\n",
        "encoder_input = layers.Input(shape=(n_steps,))\n",
        "encoder_emb = layers.Embedding(n_en_vocab, d_model)(encoder_input)\n",
        "encoder_out = EncoderLayer(d_model, n_heads)(encoder_emb)\n",
        "\n",
        "# Decoder\n",
        "decoder_input = layers.Input(shape=(n_steps,))\n",
        "decoder_emb = layers.Embedding(n_de_vocab, d_model)(decoder_input)\n",
        "decoder_out = DecoderLayer(d_model, n_heads)(decoder_emb, encoder_out, mask)\n",
        "decoder_pred = layers.Dense(n_de_vocab, activation='softmax')(decoder_out)\n",
        "\n",
        "# Model\n",
        "transformer = models.Model(\n",
        "    inputs=[encoder_input, decoder_input],\n",
        "    outputs=decoder_pred,\n",
        "    name='MiniTransformer'\n",
        ")\n",
        "transformer.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "YxaN7hF3QDv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Keunggulan Transformer**\n",
        "---\n",
        "### **A. Dibandingkan RNN/LSTM**\n",
        "* Parallel Processing: Self-attention proses semua kata sekaligus\n",
        "* Long-Term Dependencies: Tidak ada masalah \"vanishing gradient\" seperti RNN\n",
        "* Scalability: Lebih mudah di-scale untuk data besar\n",
        "\n",
        "## **B. Aplikasi**\n",
        "* Machine Translation\n",
        "* Text Summarization\n",
        "* Question Answering\n",
        "* Text Generation\n",
        "* Bahsa telah extended ke Computer Vision (Vision Transformers)"
      ],
      "metadata": {
        "id": "hgYiFRCeQCJl"
      }
    }
  ]
}